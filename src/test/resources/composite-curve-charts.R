####################################################################
#
# Meant to be run on test data generated by the SFSeize project.
# This file generates images, also in the /tmp directory, that
# describe the behavior of different composite curves on a
# shared set of queries.
#
####################################################################

require("ggplot2")


####################################################################
#
# DATA PREPARATION
#
####################################################################

# load the raw data from the unit tests from a tab-separated file
df1 <- read.csv("/tmp/composed-curves.tsv", sep="\t")

# old unit-tests mixed data series; ensure that we are only
# using the subset that pertains to query ranges
queries <- subset(df1, df1[,2] == "ranges")

# create a new column that is the curve name plus parameters
queries$full.name <- paste(queries$curve, ": ", queries$precision, " bits", sep="")

# identify the top-level curve being used:
#   H:  (Compact) Hilbert
#   R:  Row-major
#   Z:  Z-order
queries$top.curve <- substr(queries$curve, 1, 1)

# point-size used for representing precision
queries$ptsize <- sqrt(queries$precision)

# filter out the 3D and 4D queries separately
q3d <- subset(queries, dimensions==3)
q4d <- subset(queries, dimensions==4)


####################################################################
#
# FUNCTIONS
#
####################################################################

# extracts the number of dimensions as a single string;
# expect "3D" (when homogeneous) or "3,4D" (when heterogeneous)

dimensions.string <- function(df) {
  paste(paste(sort(unique(df$dimensions)), collapse=","), "D", sep="")
}

# These charts show how different composites generate different
# numbers of ranges at different rates.

range.counts_vs_time <- function() {
  ggplot() + 
    ggtitle("4D space-filling curves:  range counts v. planning time") +
    ylab("mean planning time (seconds)") + xlab("mean number of contiguous ranges") +
    scale_x_log10() + scale_y_log10() +
    geom_line(data=q4d, aes(x=avgranges, y=seconds, group=curve, color=curve)) +
    geom_point(data=q4d, aes(x=avgranges, y=seconds, group=curve, color=curve, size=ptsize)) 
  ggsave(filename="/tmp/curves-4d.png", width=16, height=5, units="in")
  
  ggplot() + 
    ggtitle("3D space-filling curves:  range counts v. planning time") +
    ylab("mean planning time (seconds)") + xlab("mean number of contiguous ranges") +
    scale_x_log10() + scale_y_log10() +
    geom_line(data=q3d, aes(x=avgranges, y=seconds, group=curve, color=curve)) +
    geom_point(data=q3d, aes(x=avgranges, y=seconds, group=curve, color=curve, size=ptsize)) 
  ggsave(filename="/tmp/curves-3d.png", width=16, height=5, units="in")
}

# These charts are meant to illustrate the trade-off between the
# compactness of the query ranges (on average, how many cells did
# each range contain) versus throughput (on average, how many total
# cells did the query-ranges return per second).
#
# To aid interpretation, the charts are generated once for 4D (XYZT)
# curves, and once for 3D (XYT) curves.  Furthermore, the charts are
# faceted horizontally by the number of plys in the curve, and 
# vertically by the top-level space-filling curve in the composite.

compactness_vs_throughput <- function() {
  ggplot() +
    ggtitle("4D space-filling curves:  compactness v. throughput") +
    xlab("cells per range (compactness)") +
    ylab("cells per second (throughput)") + 
    geom_line(data=q4d, aes(x=cells.per.range, y=cells.per.second, group=curve, color=curve), show_guide=FALSE) +
    scale_x_log10() + scale_y_log10() +
    facet_grid(top.curve ~ plys)
  ggsave(filename="/tmp/curves-4d-prorated.png", width=16, height=5, units="in")
  
  ggplot() +
    ggtitle("3D space-filling curves:  compactness v. throughput") +
    xlab("cells per range (compactness)") +
    ylab("cells per second (throughput)") + 
    geom_line(data=q3d, aes(x=cells.per.range, y=cells.per.second, group=curve, color=curve), show_guide=FALSE) +
    scale_x_log10() + scale_y_log10() +
    facet_grid(top.curve ~ plys)
  ggsave(filename="/tmp/curves-3d-prorated.png", width=16, height=5, units="in")
}

# Generates the distribution of simple scores, ordering
# them from high to low, having first grouped the runs
# by the curve topology.  The mean score per topology
# is reported.

score.plot <- function(score.label, score.column) {
  clean.data <- function(df.raw) {
    max.precision <- max(df.raw$precision)
    df.nums <- subset(df.raw, df.raw[,score.column] != Inf & df.raw$precision == max.precision)
    df.nums$use.score <- df.nums[,score.column]
    df.nums <- aggregate(df.nums, list(curve = df.nums$curve), mean)
    idxs <- order(df.nums$use.score, decreasing=TRUE)
    df.clean <- df.nums[idxs,]
    df.clean$x <- 1:nrow(df.clean)
    return(df.clean)
  }
  
  mk.chart <- function(df.raw, width, height) {
    df <- clean.data(df.raw)
    dims <- dimensions.string(df)
    
    y.max <- 10 ^ (floor(log10(max(df$use.score))) + 3.0)
    y.min <- 10 ^ (floor(log10(min(df$use.score))))
    
    print(df)
    print(dims)
    print(c(y.min, y.max))
    
    ggplot() +
      ggtitle(paste(dims, "curve", score.label, "scores at", max(df$precision), "bits, grouped by topology")) +
      scale_x_continuous(limits = c(0, nrow(df)+2, breaks=NULL)) +
      scale_y_log10(limits = c(y.min, y.max)) +
      xlab(NULL) + ylab(paste(score.label, "score")) +
      geom_point(data=df, aes(x=x, y=use.score, group=curve, color=curve), show_guide=FALSE) +
      geom_text(data=df, aes(legend=FALSE, x=x, y=use.score, label=curve, angle=45, size=1, hjust=-0.1, vjust=0), show_guide=FALSE) 
    ggsave(filename=paste("/tmp/curves-", dims, "-", score.label, "-scores.png", sep=""), width=width, height=height, units="in")
  }
  
  mk.chart(q3d, 8.0, 5.0)
  mk.chart(q4d, 20.0, 5.0)
}


range.study.plot <- function(invert = FALSE) {
  mk.chart <- function(df.raw, width, height) {
    df <- df.raw
    df$top.curve_f = factor(df$top.curve, levels=c("R", "Z", "H"), ordered=TRUE)
    df$plys_f = factor(df$plys, levels=c(2, 3, 4), labels=c("2-ply", "3-ply", "4-ply"), ordered=TRUE)
    df$label_f = factor(df$label, levels=c("----", "-YZT", "X-ZT", "XY-T", "XYZ-", "XYZT"), ordered=TRUE)
    dims <- dimensions.string(df)

    df$y = rep(0.0, nrow(df))
    for (i in 1:nrow(df)) {
      df$y[i] <- ifelse(invert, 1.0 / max(0.000001, df$seconds[i]), df$seconds[i])
    }
      
    print(df)
    
    x.range <- range(df$avg.ranges)
    y.range <- range(df$y)
    if (invert) y.range <- rev(y.range)
    ref <- data.frame(x=x.range, y=y.range)
    print(ref)
    
    y.label <- ifelse(invert, "inverse seconds", "seconds")
    graph.name <- ifelse(invert, "ranges-inv", "ranges")
    
    ggplot() +
      ggtitle(paste(dims, "range study")) +
      scale_x_log10() +
      scale_y_log10() +
      xlab("number of ranges") + 
      ylab(y.label) +
      geom_line(data=ref, aes(x=x, y=y), lty="dotted", size=0.2) +
      geom_line(data=df, aes(x=avg.ranges, y=y, group=curve, color=curve), show_guide=FALSE) +
      facet_grid(label_f ~ top.curve_f + plys_f)
    ggsave(filename=paste("/tmp/", graph.name, "-", dims, ".png", sep=""), width=width, height=height, units="in")
  }
  
  mk.chart(q3d, 8.0, 5.0)
  mk.chart(q4d, 18.0, 12.0)
}


####################################################################
#
# MAIN PROGRAM
#
####################################################################

#range.counts_vs_time
#compactness_vs_throughput()
#score.plot("adjusted", "adj.score")
#score.plot("raw", "score")
range.study.plot(FALSE)
range.study.plot(TRUE)
